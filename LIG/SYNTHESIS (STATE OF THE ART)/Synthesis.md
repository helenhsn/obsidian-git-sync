
# Energy implications of ICT (short introduction)

>[!Articles related]
>[[Consumption Categories in ICT]], [[Statistics (Data Demand)]]


### In order of effect: 

- Energy used to directly produce and use ICT
- Immediate consequences of environmental impacts (*ex: change in travel*)
- Ongoing changes as ICT is used over time, leading to other adjustments, changes and innovations

### Regarding power (electricity consumption):

- ICT is expected to grow to about **21% of our global electricity consumption** by **2030**.
	- Traffic consumption for **smartphones** is expected to **increase 10 times** for mobile data between **2016-2022**, forming **50%** of total global Internet traffic by 2022.
- **IP video traffic** is expected to form **82% of 4.75 ZB global IP traffic** by **2022**.
- There has been **considerable variance in results**, with different methods yielding very **different estimates of how much energy the Internet uses** (*what the consumption attributable to a MB of data traffic might be*).


//
--

# 1. Data Demand Analysis - *Which sectors of digital technologies consume the most?*

>[!Articles related]
>[[Consumption Categories in ICT]] , [[Statistics 2 (Data Demand)]], [[Statistics (Data Demand)]], [[Statistics (Data demand) + Suggestions]], [[Video streaming analysis]]

## Most consuming device

- **SmartTV** is the **more consuming** (regarding device production, data traffic and operational electricity demand).
- **Smartphone** is the **less consuming**.

## Most used devices

1. **Smartphone** = *68% of total time people spend online*
2. **Smartphone + Tablets** = *75% of total time people spend online*

## Most used apps

- **Watching** apps:
	1. **Youtube is the larget contributor to data demand** (consuming 49.25% of demand for **watching accross all house-holds**).
	2. **Now TV** (8.85%)
	3. **Netflix** (8.71%)
	4. **Sky** (8.26%)
	5. **TV Player** (7.37%)
	6. **Facebook videos** (2.51%)...
- **Listening** apps
- **Social networking** apps
- **Background processes** app

## Factors that increase data demand:

-   Watching **separate content via different mediums at the same time** (e.g. through broadcast TV, on-demand services, DVDs etc.)
-   The **variety of devices available** has created new possibilities for how and when watching can be carried out.


//
--

# 2. Dark Patterns -*What makes people consume more and more data?*

>[!Articles related]
>[[Dark patterns in Streaming Platforms]]


![[Dark patterns.png]]

## Feature Fog

>[!Definition]
>**Induce unawareness** by reducing autonomy of monitoring user time spent. 
>These UI patterns are designed so that the **user is less able to get feedback on time spent** engaged in a viewing session.



## Extreme countdown

>[!Definition]
> Refers to UI patterns that **have a timer** and that **execute automatically if not interrupted** within the short period of time.
> Such patterns cause unintended behaviors of over-watching : reduce user autonomy in making conscious decisions, especially as time increases in a video watching session.
> Though such a pattern is useful when watching the desired content, it is also responsible for enforcing a sunk cost fallacy when users have invested some time in something that they might not necessarily even enjoy watching.
> ex : *Autoplay in Netflix*


## Switch off delay

>[!Definition]
>UI patterns that promote strategies of **hiding restrictive usage features** in the **default** UI.
>ex : *Log out feature not readily available on many platforms* : participants pointed how they have been discouraged to log out due to the unavailability of logout on the main landing page.


## Attention quicksand

>[!Definition]
> UI patterns that **instantly start** without **conscious user action**. They instantly **grab user attention** and **divert them** from what could otherwise be a different online behavior.
> ex : *instant GIF starter on video thumbnails upon mouse hover or single touch scroll on mobile devices.* OR *Trailers of popular shows/movies starting automatically on Netflix*

>[!Important]
> These platforms use the power of **animated visual content** consumed through the path of least cognitive resistance that causes **instant gratifcation** through a simple hover interaction.


##### Bias grind

>[!Definition]
>Refers to UI patterns that **disproportionately overload user interests and biases**.
>ex : *choice overload -> providing an infinitely long scroll of Recommendations based on previous watching history*.

>[!Important]
>Recommendation features **enable compulsiveness**, especially as the **viewing time of a session increases**. Provides **so many options** that sometimes users fall into an endless list of irrelevant videos.


//
--

# 3. Targeted Behaviors - *What user behaviors are particularly relevant regarding Data Consumption?*

>[!Articles related]
> [[Statistics (Data demand) + Suggestions]], [[Video streaming analysis]]

## A. Binge-watching (compulsiveness)


## B. Media-multitasking

=> Watching **separate content via different mediums at the same time**
- Leading to the accomplishment of other activities whilst the TV is on. 
- Many users access their mobile devices whilst watching TV for both “media-meshing” i.e. **interaction involving the TV program in view**, and “media-stacking” i.e. **interaction for other means unrelated to the TV**.


## C. Other consuming habits

#### Time related

([[Statistics (Data demand) + Suggestions]])
- Watching typically occured every day of the week. There were peaks **early morning** (07:00) and **late at night** (22:00), with an **early evening peak** at 18:00.

([[Video streaming analysis]])
- Participants streame**more on weekends** than on weekdays.
- Participants streamed less in the morning, afternoon and at night than in the **evening**.
- Participants had streamed  **1.77 hours per day**.

//
--

# 4. Nudges - *Leads to hamper these behaviors*

>[!Articles related]
>[[Leads to reduce Data Demand]], [[Leads to reduce Data Demand 2]], [[Defining Digital sufficiency]], [[Examples]]

## A. Defining Digital Sufficiency

1. **Hardware sufficiency**
	- being able to **produce fewer devices**
	-   designing **devices that last** for a long time
	-   ensuring that their **complexity and resource use** do not **surpass the purpose** they are designed for ('not craking a nut with a sledge-hammer')
	-   **keeping** their absolute **energy demand** at **lowest level possible** to perform the desired tasks
	- **HOW CAN WE OBTAIN HARDWARE SUFFICIENCY?**
			-> manufacturers should **offer repair and upgrade services** and **provide necessary software updates** for operating systems until the end of a device’s physical lifetime.
			-> hardware companies should **change their business models** from **selling** to **letting** (device-as-a-service), allowing devices that do not meet the requirements of the users to be returned and redistributed to other users after refurbishment.
			-> **policies** should aim at **improving collection and recovery rates**, e.g., legislation should make **take-back programs mandatory**.

2. **Software sufficiency**
	- Aims at software being developed so that **data traffic** and **hardware utilization** during application are as **low as possible** in absolute terms.
	- **HOW CAN WE OBTAIN SOFTWARE SUFFICIENCY?**
			-> **Default settings** towards **minimal energy demand** + **website** allowing access with **slow Internet connections**.
			-> Limiting the extent of forced connectivity.
			-> Applying open standards.
			-> Mitigating hardware obsolescence by ensuring backward compatibility.
			-> **Labels for energy sufficient software** can promote environmentally friendly solutions, create an awareness of sustainability issues among software developers, and help users choose between alternatives.

3. **User sufficiency**
	- User sufficiency has **two goals** :  **using digital devices frugally** + **using ICT** that **promote sustainable lifestyles** and **enable to reduce their consumption** needs while **maintaining a decent quality of life**.
	- **HOW CAN WE OBTAIN USER SUFFICIENCY?**
			-> **Informational and educational campaigns** can increase knowledge of and awareness for the environmental impacts of hardware and software. -> *sustainability communication*
			-> **Nudges**

4. **Economic sufficiency**
	- Digitalization supporting a transition to an economy characterized not by economic growth as the primary goal but by production and consumption sufficient to serve existing societal and individual needs.
	- **HOW CAN WE OBTAIN ECONOMIC SUFFICIENCY?**
			-> Mostly via **policies targeting economic sufficiency** -> addressing **policymakers** at the **federal or communal level** in the fields of economic and labor policy, environmental policy, and firm regulation.


## B. Leads to moderate use - **How can ICT designs help user consume less?**

>[!Articles related]
>[[Leads to reduce Data Demand]], [[Defining Digital sufficiency]], [[Examples]]

### Raise awareness via feedback technologies OR infographics

>[!Important] 
>Designs could **raise awareness** of how, when or what they use technologies for—helping the participants and users who are already self-motivated to moderate their least meaningful use.

=> **LEADS**:

>Feedback could be provided as **periodic** and more aggregate monthly, weekly or daily reports via email

> Using **forecasts of predicted use** to persuade users to moderate (*ex : in the next 30 years, the amount of time you’ve spent on Netflix is going to be like 10 years’ or something*)

>Giving **more transparency** : people may be **less inclined to use the internet** if they were **aware of how their personal data was being used** by online companies.

>[!Examples- Nudges ]
> - Computer program which displayed the most-used programs from the last 30 min to the participants, showing the most-frequently used programs larger than the less frequently used ones -> https://doi.org/10.1145/2858036.2858193
> - Modified web browser to show statistics about the frequency of work versus non-work websites that the participants visited -> [https://doi.org/10.1177/1071181312561289](https://doi.org/10.1177/1071181312561289)
> - Different kinds of notifications that **alerted the participants: text messages, push notifications, and popup message dialogues** about **the extent of distracting activity use** :
> 	- Smartphone vibrations when a time quota for a distracting website (in their case, Facebook) was exceeded -> [https://doi.org/10.1145/3229434.3229463](https://doi.org/10.1145/3229434.3229463)
> 	-> 2 EXCEPTIONS:
> 				- SMS notifications in the intervention from Terry et al., which contained facts about the negative effects of media multitasking -> [https://doi.org/10.1016/j.chb.2016.08.009](https://doi.org/10.1016/j.chb.2016.08.009)
> 				- Framing of the content, where all but one intervention focused on the distracting activities. To investigate the effect of a positive framing, Y.-H. Kim et al. used visualizations for the use of productive programs like word processors, and compared this with distraction-focused visualizations ->[https://doi.org/10.1145/2858036.2858428](https://doi.org/10.1145/2858036.2858428)
> - [**‘Cold Turkey’**](https://getcoldturkey.com/)-> a software tool to block “time-wasting websites, games and applications” (e.g. at specific times or after a certain time limit has passed), **cannot be deactivated** 
> - [**StayFocusd**](https://chrome.google.com/webstore/detail/stayfocusd/laankejkbhbdhmipfmgcngdelahlfoji?hl=fr) -> a Chrome browser extension that blocks access to certain sites however, **can easily be bypassed** (e.g. by disabling the extension, or using an alternative browser such as Safari).

>[!Examples - Infographics]
> - [[ByCommuteInfographics.pdf]]


---

### Providing moments of reflection

>[!Important] 
>Focus on **positive ways to moderate**, providing “more carrots instead of less, and less sticks” - creating tools which **helped the user make “more deliberate” decisions** that are meaningful to them, rather than being dictated to by an app designer.

=> **LEADS**:

> User could select **personal mental health, sustainability and meaningfulness goals**, for which an app would then provide milestones to reach and **visualisations** of these milestones to **reflect on**.

 >**Embedding such reflections within apps**: emails and notifications would **display motivational messages**, quotes or reminders of tech-dopamine effects.

---

### Suggesting diverting alternatives

>[!Important] 
>**Options** could be provided to users to show them **how they may have, or could, ‘better spend’ their time**.

=> **LEADS**:

>**Learning a language** instead of **scrolling on Twitter**

>**Getting fit or earning money** instead of spending time on devices

>**Swapping online time** with **like-for-like activities** e.g. for entertainment, designs could suggest to users to listen to an audio book, or attend a local event to replace the online “end product, which is entertainment or fun, or just killing boredom”.

-> Using a **video recommendation app, proposing that activities could be recommended based on what he has watched** for “breaking the momentum of the streaming consumption”; e.g. fans of The Great British Bake Off (a UK cooking show) could be provided with recipes to make from the show.

---

### User senses and device sensors

=> **LEADS**:

>Using the human senses and physicality was a more **playful** aspect of design that the participants discussed.

>Device sensors were also suggested for **facial tracking** to prompt users to **look up from their screens** and to **turn off videos** if users are not concentrating on content (table facilitator).

---

### Location (see [[Location]]) data and movement

##### LOCATION

=> **LEADS**:

> A popular location for envisioning **moderated use** were **users’ workplaces**: **blocking social media apps** or **muting notifications**.

> **Blocking internet access in a workplace’s break spaces**.

> Other locations suggested for **moderating use** were **dangerous roads** or **specific rooms in the home**.

##### MOVEMENT

=> **LEADS**:

> Determine meaningful interactions via being busy (*ex: if I’m kind of on-the-go and doing other stuff I’m only likely to read, or respond to notifications that I’m interested in*)

> Moderating interactions via **movement** (*ex: hey walk for 300 metres and then you can use it again’*)

---

### Setting limits

##### BLOCKING CONTENT OR USE (LOCKOUT MECHANISMS)

- **Three different interaction lockout mechanisms (ILM):**
	- **Non-ILM** condition: users received only notifications that reminded them of their time limit.
	- **Weak-ILM** condition: the participants' phones were locked for increasing durations
	- **Strong-ILM** condition: locked the users' phones for the rest of the day after exceeding their time limit goal

>[!Examples]
> - [[Nudgit]]
>- [[MyTime]]
>- Blocked distractions during work time with the ’freedom’ software, a blacklist-based website blocker -> [https://doi.org/10.1145/3173574.3173666](https://doi.org/10.1145/3173574.3173666)
>- Pomodoro-timer intervention on the usage of distracting websites. In the Pomodoro method, users set a time interval (typically 25 min), during which they plan to work productively -> [https://doi.org/10.1145/3130932](https://doi.org/10.1145/3130932)
>- Website-blocking system with a chatbot to control a time-based blocking system for the transition from break to work -> [https://doi.org/10.1145/3290605.3300697](https://doi.org/10.1145/3290605.3300697)
>	- A break was operationalized as an absence of user input for at least 5 min. Upon resumption of work, the chatbot was activated and started disabling access to distracting websites for the next 25 min. The participants could negotiate a lift of the block by navigating through a conversation with the chatbot. The chatbot also suggested to start blocking sessions when a participant spent more than 15 min on distracting websites
>- [GoalKeeper](https://dl.acm.org/doi/10.1145/3314403)


##### LIMITING SCROLLING

=> **LEADS**:

> Limits on **“compulsive” session** use based on **scrolling speeds**.
 
>**Users having a maximum distance** in which they **could ‘scroll’** (e.g. on social media) which were compared with physical real-world distances: “‘oh you’ve ran like erm the whole perimeter of Manhattan’”.

##### LIMITING REFRESH CONTENT

=> **LEADS**:

> Reducing the number of times a user could refresh content.

##### LIMITING AT A SPECIFIC MOMENT

=> **LEADS**:

> A more **complex aspect** for **user-set limits** involved a broader understanding of **events or activities occurring within the users’ lives**—particularly for work tasks, building on knowledge of productivity tools already available.

>Machine learning algorithms could be utilised to **determine if users were on-time with their work deadlines**—**blocking streaming activities** if **not**.

>Users could **set their own limits during specific time periods**, alongside aid from the digital tool itself as it ‘learns’ the rate at which a user works, the time left for a deadline, and the amount of internet use that may be ‘acceptable’ in these constraints.

---
### Merging virtual and real-world experiences

##### Encouraging real-world interaction and support

>[!Important]
>Using real-world social interaction and support for the designs- connecting friends, family, or other users **moderating their use** to **"support each other"** or **"share your experiences"**.

=> **LEADS**:

> **‘The Healthy Internet Programme’**: an app which would **provide the user with different activities** e.g. sending a physical greeting card instead of an e-greeting or social media post.

> Integrating support from others into **social media moderation**:
	- if a user continues to **overuse a service** in a way which is no longer meaningful to them, the device could post a photo of the user to their friends with the caption **“‘Please help me, call me, let’s go out for a beer, I have an issue with social networks’”**.
	- Given the **privacy issues**, it could be a humorous, consent-driven design sent to only **a few supportive, user-selected friends**.

>[!Examples]
>- [[Lock n' LoL]]


##### Gamification and competition

>[!Important]
> Fun gamification or competition features in the designs could help moderate our data demand.

=> **LEADS**:

>  Family monitoring their social media use with the **highest user** having to **pay for a meal or donate money to charity**.

> **Rather than a monetary “fine”** -> “The Forefeit Incentive” app where a group of users (e.g. friends in a shared flat) would **commit to a forfeit such as washing up**.

> Designs were also gamified through **virtual rewards** e.g. badges, virtual money and tokens for device or service non-use.  
	- **Money and tokens** would be **“spent” on interactions**, **enforcing blocked or limited access after they were spent**. (*CF lockout mechanisms*)


>[!Examples]
> - [NUGU: A Group-based Intervention App for Improving Self-Regulation of Limiting Smartphone Use](https://sci-hub.hkvisa.net/10.1145/2675133.2675244) -> ’NUGU’ smartphone app. The participants were asked to specify a duration for which they wanted to stop using their smartphone (e.g., ‘studying for 30 minutes’). These limiting sessions were conceptualized as ‘missions’, and completing a mission gave the users virtual points. If a participant used the smartphone for anything except incoming calls, the mission failed. Participants could also form groups, start missions together, and compare their time-limiting efforts with those of their peers.
> 



##### Incentives and rewards

>[!Important]
> **REALLY IMPORTANT !!!!!**
> Design should take into consideration the need to **incentivise and reward people—keeping users motivated and happy**, as well as **changing their habits rather than just raising awareness of usage data**.

=> **LEADS**:

> **Financial incentives** were suggested but that **rewards might not necessarily have to be monetary-based** given users use search engines to plant trees or count steps on FitBit. **“there’s no economic benefit for that but people feel pretty good about themselves”**.

---

### Attenuating the user experience

##### Modifying colour, brigthness and image quality

=> **LEADS**:

>Modify certain aspects of a digital distraction to make it **still usable, but less appealing** to the participants.

>Remove non-essential parts of the distraction that are typically designed to convince the users to spend more time with an app or a website.

>**colour removal** could be a **“good compromise”** between **reducing the user’s experience** and **making them frustrated**, “**gradually** removing colour” the longer that users spend time on the service or device. 
>(*ex: "if you turn it into black and white, it’ll neurologically be less of an incentive to use that device”*)

>**Turning down the screen’s brightness**.

>**Removing “access to photos or videos or something dynamic**, that might eventually reduce your interest in an app”.  

>[!Examples]
>- Deisgn that switch the participants' smartphone displays to grayscale in order to reduce gratification from distracting activities. The grayscale filter was an integrated feature of their participants' smartphones, and the filter was active at all times and for all apps. -> [https://doi.org/10.1080/03623319.2020.1737461](https://doi.org/10.1080/03623319.2020.1737461)
>- Web browser tab bar so that tabs that were classified as work-related had their colour contrast enhanced, and were made larger. Tabs from non-work URLs were made smaller and always displayed on the right -> [https://doi.org/10.1177/1071181312561289](https://doi.org/10.1177/1071181312561289)
>- ’Habitlab’, a browser extension that rotates behaviour change interventions for distracting websites, that is, the participants experienced different interventions between visits to the same site -> [https://doi.org/10.1145/3274364](https://doi.org/10.1145/3274364)
>- Browser extension to remove the newsfeed from the Facebook homepage. ->  [https://doi.org/10.1145/3313831.3376672](https://doi.org/10.1145/3313831.3376672)
>- [Time Sidekick](https://sci-hub.hkvisa.net/10.1145/3411763.3451843) (see [[Reducing Risk in Digital Self-Control Tools]])


##### Preventing interactions

>[!Important]
>The **ease of access** to devices and services, and notifications, were seen to **pull users into use sessions**.

=> **LEADS**:

> **notifications could be muted or delayed**

> designers could make it **“tricky to get to the website”**

> **slowing internet connections** e.g. after a user has “been using a device/service for a certain period of time”

> “**close the app after a certain amount of time**” to prompt users to do something else

> proposed that **social media access** could **purposely drain more device battery** so users would prioritise their meaningful interactions.


##### Barriers to re-entry (IMPORTANT : See [[Reducing Risk in Digital Self-Control Tools]])

>[!Important]
>Users often check and use their devices or specific services, often just because **devices are “there”** or perhaps due to **fears of missing out online content**.
>- Barriers to re-entry may **provide data reduction opportunities** by **removing the ability to continuously refresh content** when **revisiting an application**.

=> **LEADS**:

>**Internet speed bumps as a barriers to (re-)entry :**
>- The participants’ accounts show that they can find it **“too easy” to access**, and become **distracted by, digital devices and services**. In fact, some **purposefully try to avoid** using certain services that they find **difficult to extract themselves from**, and have even introduced processes to enforce an entry barrier.

>**Internet speed bumps** could also be “more physical”, for example by identifying certain workplace locations for disconnection.

>Dedicated **spatial “internet-free zones”** could be introduced in e.g. **quiet zones** or **break spaces at work**, or when family are collocated in the home.

>[!Important]
> **Conscious interventions**, such as those that **intentionally remove elements**, **show notifcations**, or block sites—may be effective, but may also cause negative feelings in users, such as  **feelings of helplessness or annoyance** (therefore increasing the risk of abandonment)

=> **LEADS**:

>Solution : interventions that are **continuously variable** , or that **can be scaled** from a level of 0 (of) to a level of progressively higher effectiveness.
		-> remove some suggestions instead of the full sidebar of suggestions
		-> use delays before blocks
		-> smaller notifcations instead of larger notifcations

> To reduce risk of the failure of one particular intervention, a design could instead **obligate users** to adopt a **bundle of interventions**, instead of allowing a user to adopt a **single intervention alone**.


##### Provide finite content

>[!Important]
>The participants proposed **adapting online content** to be more **finite** e.g. to **prevent infinite, meaningless scroll** on news articles, social media.
>Filling users’ social feeds with **“older stuff more and more, with very little new content”**.

=> **LEADS**:

>Finite content designs** are **already offered by Instagram and Twitter** -> Designs could **produce new content at specific time intervals to ‘drip feed’ content to the user**.

>For **watching**, maybe Netflix need to **stop releasing the whole season at one time**, to **avoid** negative effects e.g. **binge watching**.

>Designs could **‘trick’ users into thinking there is no new content**—“misleading the user for the greater good”. This was seen as a good way to fool the user rather than explicitly forbidding access.

---

### Interactive designs

>[!Important]
>The designs described above could be merged to **develop a more complete design** that **moderates use** and **centralises meaningful interactions**.
>
> **Categories can be combined together** (ex : limits and rewards).

=> **LEADS**:  (**IMPORTANT !!!)**

> Users could **accrue points for not accessing certain apps** and **lose them when they did (gamification)**; once points were diminished, **users’ bandwidth would be slowed on specific services** (preventing interactions) or the **use of those services would be restricted** (setting limits); and users would receive a **summary of their use** and **what they could have done instead** if they had put their time into something else (awareness of use and suggesting alternatives)

---
### Implementing Users' Freedom of Choice

=> **LEADS:**

> Moderations of users’ Internet use should only affect users in positive ways, i.e. **adapt Internet uses** when **users are happy for their Internet uses to be adapted**.

> This requires sophisticated contextual awareness, meaning that Internet use should only be facilitated:
	1. At a **time suitable for the users**.
	2. At a **place** where **users can cope with Internet limits**.
	3. For **services** that do **not disrupt their necessary uses** of the Internet.

>[!Important]
>-> Finding the **balance between helping users** and **acting on their behalf** (e.g. limiting their use) is going to be **challenging**.
>-> If the **moderating design** is **too easily ignored**, it will be **too easy** for participants to **revert to their usage norms**.



## C. Type of nudges encountered

### Medium (from the most frequently found)
- ##### Applications -  Browser Extensions

### Categories (from the most frequently found)
- ##### Awareness features (over-represented -> easy to implement)
- ##### Lockout mechanisms - Time-based goal settings
- ##### Feedback applications
- ##### Social applications

//
--

# 5. Effectiveness/Outcome - *Any designs that proved to be efficient?*

>[!Articles related]
>[[Leads to reduce Data Demand 2]], [[Examples]]

## How can we evaluate Moderate Internet Use Designs? (see article in [[Leads to reduce Data Demand 2]] )

>[!Important]
>- Any design proposals must **clearly be tested** and **evaluated** with **end-users** to determine their **efficiency**, ideally for a sufficiently **long time** to understand genuine adoption.
>
>- Moderate Internet use tools could also **gather useful interaction** and **quantitative data** to aid in its evaluation.
>  
>- **Software based interventions** (ex: make smartphone vibrate when time limit of use is exceeded) are a **potentially effective way** of gathering longitudinal, largescale data from a wide audience. **BUT** **current restraints** that mobile operating systems place to preserve energy, ensure inter-application security and the privacy of user data, are making these types of **‘background’ software increasingly difficult to implement**.
>	- Only **specific types of apps can run continuously in the background** (e.g. music players, location trackers) on Apple’s iOS. Logging device actions and data demand in real-time is **no longer possible unless ‘disguised’ as a permitted background-execution app**.
>	- **Access and control of certain device settings** (e.g. turning off mobile data and Wi-Fi) to save upstream network demand is **restricted on Android and iOS** without breaking the standard operating systems protections.


## Awareness Interventions (see article in [[Examples]])

- Awareness interventions have led to reductions in time spent on distractions and time on the device both through the display of usage statistics and notifications
- **Awareness notifications** only achieved **positive effects when they were insistent**.
- For usage monitoring via notifications, no effect was observed on total time on device.
- **Regular messages** about the **negative effects of excessive media consumption** also had **no positive effects**. 
- Only the **smartphone vibrations** to remind of **overuse led to less time spent** on the distraction with **personalized** and **static time quotas**.

## Goal-advancement interventions (see article in [[Examples]])

- **Time-based goal setting** with **warning prompts** led to **reductions in time** on device, and the amount of time spent on distractions in one intervention.
- In other interventions, these **prompts of exceeding a time limit** did **not result in a reduction in time** on device **until** the **excess was sanctioned**.
- When sanctions in the form of **device lockout** were added, time on device was reduced.
- When participants **set action goals in addition to time goals**, they were **more likely to follow prompts** to **leave a distraction**.
- Changes in the duration of individual sessions were not significant.

>[!Important]
>- **Setting action goals immediately before the start of a distracting activity** also led to a **large reduction in total time** spent on the distraction and the frequency of distraction starts.

## Blocking intervention (see article in [[Examples]])

>[!Important]
>- **Blocking the access to distractions** with a task convinced participants **to not start distractions proportionally to the task difficulty**: 
>	- Requiring a 30-digit input before opening an app was **more effective** than a **10-digit input task** or a **simple warning prompt**.
>- **HOWEVER** : this task-based delay was the only type of intervention for which an increase in time on distraction was reported, indicating that participants **compensated for the initial hurdle of opening an app** by **spending more time in it**.
>- Still, the aspect of **being able to negotiate the lift of a block** was **preferred to time-based blocking**, and participants activated the intervention more often and started distraction less often than with purely time-based blocking intervention. (**SEE IMPLEMENTING USERS' FREEDOM OF CHOICE **)


## Content modification intervention (see article in [[Examples]])

- **Setting the smartphone display to grayscale** led to a **reduction in total phone use**, **social media use**, and **internet browser use**. **No effect on video player use** was found.
- **Removing the _Facebook_ newsfeed** resulted in a **reduction for the duration of an individual session**. **No effects** were found for the **total time spent on the distraction** or the **starts of the distracting activity**.

## Reward intervention (see article in [[Examples]])

>[!Important]
>***REALLY IMPORTANT:* When **social support features** were added to a **goal-advancement intervention**, participants **set significantly more time-limit goals per day**, **reduced their time** on the device, and **started distractions less often** . These changes were not observed in the variant without the social support features#2b.

## Conclusion (see article in [[Examples]])

>[!Important]
>- The **insistence** with which an intervention attempts to convince the users, and the **ease with which participants can dismiss interventions** appear to be **relevant factors for the success of interventions**:
>
>1. **Relying solely** on the **users monitoring and adapting** their own behaviour **is not effective**.
>	- Greater effects reported for the beginning of their intervention, which suggests that a novelty effect could be at least partially responsible for the effectiveness of interventions while habitual behaviour may gain the upper hand again in the future.
>	- Interventions that users can **easily ignore** **do not achieve the desired results**.
>2. Interventions that are **more insistent** in grabbing the users' attention at times of excessive distraction use are **more effective**.

=> This balancing act requires the ability to have **adaptive sanctioning strictness**.


//
--

# 6. [[Table of references]]

